![image-20240407202132117](https://github.com/jiangsu415/yolo_world/assets/130949548/bfa3052d-f8f0-4109-9ba4-58a163072627)**创新点：**1.可重参数化的视觉语言路径聚合网络和区域文本对比损失

​				3.新的预训练方案：将检测数据、定位数据和图像文本数据统一为区域文本对

​				3.Promopt-and-detect "先提示后检测"：首先对用户提示进行编码，建立离线词汇，该词汇对离线需求不断变化，然后在推理时可以在运行时推断出离线词汇，而无需对提示进行编码

​				4.包含在1内的创新点：**文本引导的CSPLayer（T-CSPLayer）和图像池化注意力**（I-Pooling Attention）加强图像与文本特征之间的交互。

**可创新点：**1.小型检测器进行预训练以赋予其开放识别的能力（以往都是采用一些重型的网络结构）

​					2.YOLOV8

​					3.T-CSPlayer结构

**结构：**图片检测分支的backbone为YOLOV8，在训练时，对输入文本采用预训练过的CLIP进行编码（检测时舍弃编码器，使用文本嵌入可重新参数化的REPvl-PAN的权重），使用REPVL-PAN链接语言文本和图像特征，之后采用了一个文本对比头部包含两个3*3的卷积来回归边界框和对象嵌入（用于视觉-语言模型中的一个组件，用于对比学习。它的目标是通过学习区分正样本和负样本对，从而使视觉和文本表示相互对齐。在视觉-语言任务中，正样本对由一张图像和其对应的文本描述或标题组成，而负样本对则由一张图像和不同的文本描述组成。其目标是鼓励模型将相似的视觉和文本表示映射到嵌入空间中的相近位置，同时将不相似的表示推开）和box-head用于预测边界框和类别等。

*损失函数：遵循[20]并利用任务对齐的标签分配[9]将预测与真实注释进行匹配，并将每个正预测分配一个文本索引作为分类标签。基于对象-文本（区域文本的相似度）和对象文本分配的交叉熵构建区域文本对比损失，和IOU和分布焦点损失进行文本框回归*

**个人理解：**

离线词汇：可能是在推理时，根据用户的输入匹配已经编码好的文献词汇表，在里面查找，比如说“在雪地上，有一只狗”，在词汇表中匹配狗这个名词，即使模型在训练时没有标签中没有狗这个名词，根据文本嵌入可以找出狗这个目标。离线词汇表可以避免为每个输入进行计算，并提供根据需要调整词汇表的灵活性。

可重新参数化：在推理时，将离线词汇表嵌入到卷积层或者BN层

T-CSPLayer：先经过一个Dark bottleneck网络结构进行一个图像特征提取，之后利用max-sigmoid融合图像和文本特征，最终再连接其他尺度的特征：

I-pooling Attention：类似于YOLOV5和YOLOV7的多尺度通过不同尺寸的卷积去做池化然后拼接在一起后再连接一共MHCA，可能是注意力

Image-aware Embedding：图像感知嵌入联合捕捉图像和文本特征

**后续需要学习的地方：**YOLOV8，CLIP，Dark Bottleneck，Max-sigmoid注意力，图像池化注意力
![image-20240326150553574](https://github.com/jiangsu415/yolo_world/assets/130949548/2acc968c-81ec-44db-8727-46110012c3de)

区域文本对比损失：就是视觉提出来的候选框是文本想要的损失就低不是就高

区域—文本对，将一些类别变成文本的描述

![image-20240407200200237](https://github.com/jiangsu415/yolo_world/assets/130949548/1c14a5f5-c692-4d0e-bdf4-9cfd412b0386)

输出为预测框和Emedding，当选出一个候选框后其内的物体做成Embedding向量，然后与输入的文本特征向量看一个相似度是属于哪一个类别

![image-20240407200807057](https://github.com/jiangsu415/yolo_world/assets/130949548/de59b6a1-c85f-4eaf-953b-523f17b42357)

文本和图像互相增强，文本与图像要经过一个跨模态的融合，类似于视觉cross-attention的方法，图像的每一块特征图中都与文本的token去计算关系
![image-20240407202132117](https://github.com/jiangsu415/yolo_world/assets/130949548/f2b6c955-c758-4977-a0ea-54c9a0310b1d)

输出是一个坐标框的回归和匹配文本，预测的Embedding与哪一个文本比较相似，和哪个文本比较相似就是哪一个

“我想找一只狗”他会提出来狗这一名词，并编码为一个D维的特征向量，只提取一些关键的名词.并且这个文本编码器能更好的适应图像特征

![image-20240407211602873](https://github.com/jiangsu415/yolo_world/assets/130949548/750d5c53-1970-43c9-b094-dee92dd04c84)

回归的文本框和物体编码，wj是文本的特征，ek是物体的embedding，先做一个norm，然后每一个去计算内积匹配，

![image-20240409150028687](https://github.com/jiangsu415/yolo_world/assets/130949548/721b05f9-d688-4577-8ca1-81374dd285f1)

先人为写一些提示，然后把文本做编码，获取到文本编码，之后做一些匹配

RepL-VAN中的max-sigmoid是需要每一个特征图上的点去和这三个文本编码特征做匹配，一旦与这三个本文一个有关系就保留这个点。通过本文的特征编码来重新整合一下，特征图中哪些区域是我需要特别关注的，将他的权重值设置的更大
![image-20240409151830250](https://github.com/jiangsu415/yolo_world/assets/130949548/d291297e-9aed-4a06-8aa8-187dc6bf7a45)

做cross-Attention是序列和序列做，但是图像是特征图维度不匹配，所以要把特征图转化为一个特征序列，所以他先做一个不同尺度的一个池化把特征图大小都转化为3*3的大小后，把这个三个不同尺度的特征图做成一个序列一维的序列，去和文本特征做一个匹配和更新
![image-20240409152708483](https://github.com/jiangsu415/yolo_world/assets/130949548/3a79f77b-c34b-4593-b6dd-1cfeeff693be)

之后像输入三个不同尺度的特征，链接一个yolo-head做一个boundingbox，另外他还输出一个物体的embedding去计算和哪个文本序列是相似的，和哪个相似度越大就说属于哪个类别

![image-20240409153824682](https://github.com/jiangsu415/yolo_world/assets/130949548/774af7c4-fbe0-44ee-ab28-5749f8f5ec4c)

文本矫正后，得到文本加权后的特征图

![image-20240409153721151](https://github.com/jiangsu415/yolo_world/assets/130949548/6b251b5f-dceb-43bb-abe4-fdf2ccb0db77)

区域文本对比损失，图像的object embedding和文本做一个匹配，和哪个匹配上就是哪个。

伪标签，图像文本配对，提示的文本。
